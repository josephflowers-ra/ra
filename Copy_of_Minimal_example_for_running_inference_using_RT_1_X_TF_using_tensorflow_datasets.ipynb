{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josephflowers-ra/ra/blob/rt1x/Copy_of_Minimal_example_for_running_inference_using_RT_1_X_TF_using_tensorflow_datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uFaKTTg7Dy3t"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2020 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4n4VANfTrsE"
      },
      "outputs": [],
      "source": [
        "# Install required library\n",
        "\n",
        "# Using tfp-nightly due to https://github.com/tensorflow/probability/issues/1752\n",
        "!pip install rlds tf_agents dm-reverb[tensorflow] apache_beam tfp-nightly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download zipped checkpoint folder\n",
        "\n",
        "!gsutil -m cp -r gs://gdm-robotics-open-x-embodiment/open_x_embodiment_and_rt_x_oss/rt_1_x_tf_trained_for_002272480_step.zip ."
      ],
      "metadata": {
        "id": "5amT3YssuaHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip zipped checkpoint folder\n",
        "\n",
        "!unzip rt_1_x_tf_trained_for_002272480_step.zip"
      ],
      "metadata": {
        "id": "wOh2fyQ9u3qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import rlds\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "import tf_agents\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from IPython import display\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def as_gif(images):\n",
        "  # Render the images as the gif:\n",
        "  images[0].save('/tmp/temp.gif', save_all=True, append_images=images[1:], duration=1000, loop=0)\n",
        "  gif_bytes = open('/tmp/temp.gif','rb').read()\n",
        "  return gif_bytes\n"
      ],
      "metadata": {
        "id": "Q2n8xIRPvD00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load TF model checkpoint\n",
        "# Replace saved_model_path with path to the parent folder of\n",
        "# the folder rt_1_x_tf_trained_for_002272480_step.\n",
        "saved_model_path = 'rt_1_x_tf_trained_for_002272480_step'\n",
        "\n",
        "tfa_policy = py_tf_eager_policy.SavedModelPyTFEagerPolicy(\n",
        "    model_path=saved_model_path,\n",
        "    load_specs_from_pbtxt=True,\n",
        "    use_tf_function=True)"
      ],
      "metadata": {
        "id": "s3348y2FTtAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one step of inference using dummy input\n",
        "\n",
        "# Obtain a dummy observation, where the features are all 0\n",
        "observation = tf_agents.specs.zero_spec_nest(tf_agents.specs.from_spec(tfa_policy.time_step_spec.observation))\n",
        "\n",
        "# Construct a tf_agents time_step from the dummy observation\n",
        "tfa_time_step = ts.transition(observation, reward=np.zeros((), dtype=np.float32))\n",
        "\n",
        "# Initialize the state of the policy\n",
        "policy_state = tfa_policy.get_initial_state(batch_size=1)\n",
        "\n",
        "# Run inference using the policy\n",
        "action = tfa_policy.action(tfa_time_step, policy_state)"
      ],
      "metadata": {
        "id": "iT79xAACTvNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset object to obtain episode from\n",
        "\n",
        "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
        "ds = builder.as_dataset(split='train[:1]')\n",
        "\n",
        "ds_iterator = iter(ds)"
      ],
      "metadata": {
        "id": "z0cRt78HTxAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the steps from one episode from the dataset\n",
        "\n",
        "episode = next(ds_iterator)\n",
        "steps = episode[rlds.STEPS]"
      ],
      "metadata": {
        "id": "xssMOtFtTx-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "\n",
        "for step in steps:\n",
        "\n",
        "  im = Image.fromarray(np.array(step['observation']['image']))\n",
        "  images.append(im)\n",
        "\n",
        "print(f'{len(images)} images')\n",
        "\n",
        "display.Image(as_gif(images))"
      ],
      "metadata": {
        "id": "1_2oB0-FT0HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(image):\n",
        "  image = tf.image.resize_with_pad(image, target_width=320, target_height=256)\n",
        "  image = tf.cast(image, tf.uint8)\n",
        "  return image\n",
        "\n",
        "def terminate_bool_to_act(terminate_episode: tf.Tensor) -> tf.Tensor:\n",
        "  return tf.cond(\n",
        "      terminate_episode == tf.constant(1.0),\n",
        "      lambda: tf.constant([1, 0, 0], dtype=tf.int32),\n",
        "      lambda: tf.constant([0, 1, 0], dtype=tf.int32),\n",
        "  )\n",
        "\n",
        "def rescale_action_with_bound(\n",
        "    actions: tf.Tensor,\n",
        "    low: float,\n",
        "    high: float,\n",
        "    safety_margin: float = 0,\n",
        "    post_scaling_max: float = 1.0,\n",
        "    post_scaling_min: float = -1.0,\n",
        ") -> tf.Tensor:\n",
        "  \"\"\"Formula taken from https://stats.stackexchange.com/questions/281162/scale-a-number-between-a-range.\"\"\"\n",
        "  resc_actions = (actions - low) / (high - low) * (\n",
        "      post_scaling_max - post_scaling_min\n",
        "  ) + post_scaling_min\n",
        "  return tf.clip_by_value(\n",
        "      resc_actions,\n",
        "      post_scaling_min + safety_margin,\n",
        "      post_scaling_max - safety_margin,\n",
        "  )\n",
        "\n",
        "def rescale_action(action):\n",
        "  \"\"\"Rescales action.\"\"\"\n",
        "\n",
        "  action['world_vector'] = rescale_action_with_bound(\n",
        "      action['world_vector'],\n",
        "      low=-0.05,\n",
        "      high=0.05,\n",
        "      safety_margin=0.01,\n",
        "      post_scaling_max=1.75,\n",
        "      post_scaling_min=-1.75,\n",
        "  )\n",
        "  action['rotation_delta'] = rescale_action_with_bound(\n",
        "      action['rotation_delta'],\n",
        "      low=-0.25,\n",
        "      high=0.25,\n",
        "      safety_margin=0.01,\n",
        "      post_scaling_max=1.4,\n",
        "      post_scaling_min=-1.4,\n",
        "  )\n",
        "\n",
        "  return action\n",
        "\n",
        "def to_model_action(from_step):\n",
        "  \"\"\"Convert dataset action to model action. This function is specific for the Bridge dataset.\"\"\"\n",
        "\n",
        "  model_action = {}\n",
        "\n",
        "  model_action['world_vector'] = from_step['action']['world_vector']\n",
        "  model_action['terminate_episode'] = terminate_bool_to_act(\n",
        "      from_step['action']['terminate_episode']\n",
        "  )\n",
        "\n",
        "  model_action['rotation_delta'] = from_step['action']['rotation_delta']\n",
        "\n",
        "  open_gripper = from_step['action']['open_gripper']\n",
        "\n",
        "  possible_values = tf.constant([True, False], dtype=tf.bool)\n",
        "  eq = tf.equal(possible_values, open_gripper)\n",
        "\n",
        "  assert_op = tf.Assert(tf.reduce_any(eq), [open_gripper])\n",
        "\n",
        "  with tf.control_dependencies([assert_op]):\n",
        "    model_action['gripper_closedness_action'] = tf.cond(\n",
        "        # for open_gripper in bridge dataset,\n",
        "        # 0 is fully closed and 1 is fully open\n",
        "        open_gripper,\n",
        "        # for Fractal data,\n",
        "        # gripper_closedness_action = -1 means opening the gripper and\n",
        "        # gripper_closedness_action = 1 means closing the gripper.\n",
        "        lambda: tf.constant([-1.0], dtype=tf.float32),\n",
        "        lambda: tf.constant([1.0], dtype=tf.float32),\n",
        "    )\n",
        "\n",
        "  model_action = rescale_action(model_action)\n",
        "\n",
        "  return model_action"
      ],
      "metadata": {
        "id": "yRBD9VmaT1d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "steps = list(steps)\n",
        "\n",
        "policy_state = tfa_policy.get_initial_state(batch_size=1)\n",
        "\n",
        "gt_actions = []\n",
        "predicted_actions = []\n",
        "images = []\n",
        "\n",
        "for step in steps:\n",
        "\n",
        "  image = resize(step[rlds.OBSERVATION]['image'])\n",
        "\n",
        "  images.append(image)\n",
        "  observation['image'] = image\n",
        "\n",
        "  tfa_time_step = ts.transition(observation, reward=np.zeros((), dtype=np.float32))\n",
        "\n",
        "  policy_step = tfa_policy.action(tfa_time_step, policy_state)\n",
        "  action = policy_step.action\n",
        "  policy_state = policy_step.state\n",
        "\n",
        "  predicted_actions.append(action)\n",
        "  gt_actions.append(to_model_action(step))"
      ],
      "metadata": {
        "id": "6Wwo5cdhT23A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action_name_to_values_over_time = defaultdict(list)\n",
        "predicted_action_name_to_values_over_time = defaultdict(list)\n",
        "figure_layout = ['terminate_episode_0', 'terminate_episode_1',\n",
        "        'terminate_episode_2', 'world_vector_0', 'world_vector_1',\n",
        "        'world_vector_2', 'rotation_delta_0', 'rotation_delta_1',\n",
        "        'rotation_delta_2', 'gripper_closedness_action_0']\n",
        "action_order = ['terminate_episode', 'world_vector', 'rotation_delta', 'gripper_closedness_action']\n",
        "\n",
        "for i, action in enumerate(gt_actions):\n",
        "\n",
        "  for action_name in action_order:\n",
        "\n",
        "    for action_sub_dimension in range(action[action_name].shape[0]):\n",
        "\n",
        "      # print(action_name, action_sub_dimension)\n",
        "      title = f'{action_name}_{action_sub_dimension}'\n",
        "\n",
        "      action_name_to_values_over_time[title].append(action[action_name][action_sub_dimension])\n",
        "      predicted_action_name_to_values_over_time[title].append(predicted_actions[i][action_name][action_sub_dimension])"
      ],
      "metadata": {
        "id": "iFiOcLQKT6Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure_layout = [\n",
        "    ['image'] * len(figure_layout),\n",
        "    figure_layout\n",
        "]\n"
      ],
      "metadata": {
        "id": "YCcJv9XlT7Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams.update({'font.size': 12})\n",
        "\n",
        "stacked = tf.concat(tf.unstack(images[::3], axis=0), 1)\n",
        "\n",
        "fig, axs = plt.subplot_mosaic(figure_layout)\n",
        "fig.set_size_inches([45, 10])\n",
        "\n",
        "for i, (k, v) in enumerate(action_name_to_values_over_time.items()):\n",
        "\n",
        "  axs[k].plot(v, label='ground truth')\n",
        "  axs[k].plot(predicted_action_name_to_values_over_time[k], label='predicted action')\n",
        "  axs[k].set_title(k)\n",
        "  axs[k].set_xlabel('Time in one episode')\n",
        "\n",
        "axs['image'].imshow(stacked.numpy())\n",
        "axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
        "\n",
        "plt.legend()\n"
      ],
      "metadata": {
        "id": "g0a7bVucT_fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNvEcl3jznXD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}